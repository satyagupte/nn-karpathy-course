{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "90570dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/satyajitgupte/Downloads/input.txt', 'r', encoding='utf-8') as infile:\n",
    "    text = infile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f76eb83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Chars in Text = 1115394\n"
     ]
    }
   ],
   "source": [
    "print(f'Num Chars in Text = {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8654ed47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "821567ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8513d240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7f14acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8d149e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda s: [stoi[ch] for ch in s]\n",
    "decode = lambda i: ''.join([itos[ix] for ix in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "65677551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46, 47, 1, 58, 46, 43, 56, 43]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('hi there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e70f2a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi there\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode('hi there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eaeb3762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "005e2d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode text\n",
    "data = torch.tensor(encode(text), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "21cc2d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9f00e187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bdb44df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and val\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b0a8f179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1003854]) torch.Size([111540])\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "038ad05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5e9ec3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target is 47\n",
      "when input is tensor([18, 47]) the target is 56\n",
      "when input is tensor([18, 47, 56]) the target is 57\n",
      "when input is tensor([18, 47, 56, 57]) the target is 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is {context} the target is {target}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "133fcf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8eff14e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data) - batch_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+1+block_size] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ea4b5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = get_batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9628e98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs =  torch.Size([32, 32]) \n",
      " tensor([[58, 53,  1,  ..., 43, 52,  1],\n",
      "        [49,  1, 39,  ..., 46, 43,  1],\n",
      "        [59, 50, 42,  ...,  0, 35, 43],\n",
      "        ...,\n",
      "        [17, 24, 27,  ..., 47, 51,  1],\n",
      "        [ 5, 57,  1,  ..., 54, 56, 53],\n",
      "        [46,  1, 39,  ..., 53, 59,  1]])\n"
     ]
    }
   ],
   "source": [
    "print(\"inputs = \", xb.shape,\"\\n\", xb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f90b9d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs =  torch.Size([32, 32]) \n",
      " tensor([[ 1, 54, 56,  ..., 50, 43, 57],\n",
      "        [43, 39, 52,  ..., 53, 60, 43],\n",
      "        [ 1, 47, 57,  ..., 53, 59, 57],\n",
      "        ...,\n",
      "        [ 1, 40, 43,  ..., 46, 43,  1],\n",
      "        [ 0, 32, 46,  ...,  1, 40, 43],\n",
      "        [41, 43,  6,  ..., 43, 56,  8]])\n"
     ]
    }
   ],
   "source": [
    "print(\"outputs = \", yb.shape,\"\\n\", yb )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "91ddd729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [43] target is 1\n",
      "when input is [43, 1] target is 54\n",
      "when input is [43, 1, 54] target is 56\n",
      "when input is [43, 1, 54, 56] target is 47\n",
      "when input is [43, 1, 54, 56, 47] target is 52\n",
      "when input is [43, 1, 54, 56, 47, 52] target is 41\n",
      "when input is [43, 1, 54, 56, 47, 52, 41] target is 43\n",
      "when input is [43, 1, 54, 56, 47, 52, 41, 43] target is 5\n",
      "when input is [51] target is 43\n",
      "when input is [51, 43] target is 39\n",
      "when input is [51, 43, 39] target is 52\n",
      "when input is [51, 43, 39, 52] target is 57\n",
      "when input is [51, 43, 39, 52, 57] target is 1\n",
      "when input is [51, 43, 39, 52, 57, 1] target is 58\n",
      "when input is [51, 43, 39, 52, 57, 1, 58] target is 53\n",
      "when input is [51, 43, 39, 52, 57, 1, 58, 53] target is 1\n",
      "when input is [46] target is 1\n",
      "when input is [46, 1] target is 47\n",
      "when input is [46, 1, 47] target is 57\n",
      "when input is [46, 1, 47, 57] target is 1\n",
      "when input is [46, 1, 47, 57, 1] target is 58\n",
      "when input is [46, 1, 47, 57, 1, 58] target is 46\n",
      "when input is [46, 1, 47, 57, 1, 58, 46] target is 43\n",
      "when input is [46, 1, 47, 57, 1, 58, 46, 43] target is 1\n",
      "when input is [5] target is 58\n",
      "when input is [5, 58] target is 61\n",
      "when input is [5, 58, 61] target is 47\n",
      "when input is [5, 58, 61, 47] target is 62\n",
      "when input is [5, 58, 61, 47, 62] target is 58\n",
      "when input is [5, 58, 61, 47, 62, 58] target is 1\n",
      "when input is [5, 58, 61, 47, 62, 58, 1] target is 61\n",
      "when input is [5, 58, 61, 47, 62, 58, 1, 61] target is 47\n",
      "when input is [45] target is 1\n",
      "when input is [45, 1] target is 20\n",
      "when input is [45, 1, 20] target is 43\n",
      "when input is [45, 1, 20, 43] target is 52\n",
      "when input is [45, 1, 20, 43, 52] target is 56\n",
      "when input is [45, 1, 20, 43, 52, 56] target is 63\n",
      "when input is [45, 1, 20, 43, 52, 56, 63] target is 5\n",
      "when input is [45, 1, 20, 43, 52, 56, 63, 5] target is 57\n",
      "when input is [42] target is 1\n",
      "when input is [42, 1] target is 21\n",
      "when input is [42, 1, 21] target is 11\n",
      "when input is [42, 1, 21, 11] target is 1\n",
      "when input is [42, 1, 21, 11, 1] target is 39\n",
      "when input is [42, 1, 21, 11, 1, 39] target is 52\n",
      "when input is [42, 1, 21, 11, 1, 39, 52] target is 42\n",
      "when input is [42, 1, 21, 11, 1, 39, 52, 42] target is 6\n",
      "when input is [51] target is 53\n",
      "when input is [51, 53] target is 56\n",
      "when input is [51, 53, 56] target is 56\n",
      "when input is [51, 53, 56, 56] target is 53\n",
      "when input is [51, 53, 56, 56, 53] target is 61\n",
      "when input is [51, 53, 56, 56, 53, 61] target is 1\n",
      "when input is [51, 53, 56, 56, 53, 61, 1] target is 63\n",
      "when input is [51, 53, 56, 56, 53, 61, 1, 63] target is 53\n",
      "when input is [1] target is 61\n",
      "when input is [1, 61] target is 53\n",
      "when input is [1, 61, 53] target is 51\n",
      "when input is [1, 61, 53, 51] target is 39\n",
      "when input is [1, 61, 53, 51, 39] target is 52\n",
      "when input is [1, 61, 53, 51, 39, 52] target is 1\n",
      "when input is [1, 61, 53, 51, 39, 52, 1] target is 47\n",
      "when input is [1, 61, 53, 51, 39, 52, 1, 47] target is 52\n",
      "when input is [43] target is 1\n",
      "when input is [43, 1] target is 42\n",
      "when input is [43, 1, 42] target is 59\n",
      "when input is [43, 1, 42, 59] target is 57\n",
      "when input is [43, 1, 42, 59, 57] target is 58\n",
      "when input is [43, 1, 42, 59, 57, 58] target is 8\n",
      "when input is [43, 1, 42, 59, 57, 58, 8] target is 0\n",
      "when input is [43, 1, 42, 59, 57, 58, 8, 0] target is 0\n",
      "when input is [1] target is 46\n",
      "when input is [1, 46] target is 39\n",
      "when input is [1, 46, 39] target is 52\n",
      "when input is [1, 46, 39, 52] target is 42\n",
      "when input is [1, 46, 39, 52, 42] target is 57\n",
      "when input is [1, 46, 39, 52, 42, 57] target is 1\n",
      "when input is [1, 46, 39, 52, 42, 57, 1] target is 61\n",
      "when input is [1, 46, 39, 52, 42, 57, 1, 61] target is 47\n",
      "when input is [56] target is 57\n",
      "when input is [56, 57] target is 46\n",
      "when input is [56, 57, 46] target is 39\n",
      "when input is [56, 57, 46, 39] target is 42\n",
      "when input is [56, 57, 46, 39, 42] target is 43\n",
      "when input is [56, 57, 46, 39, 42, 43] target is 57\n",
      "when input is [56, 57, 46, 39, 42, 43, 57] target is 1\n",
      "when input is [56, 57, 46, 39, 42, 43, 57, 1] target is 46\n",
      "when input is [0] target is 14\n",
      "when input is [0, 14] target is 59\n",
      "when input is [0, 14, 59] target is 58\n",
      "when input is [0, 14, 59, 58] target is 1\n",
      "when input is [0, 14, 59, 58, 1] target is 44\n",
      "when input is [0, 14, 59, 58, 1, 44] target is 53\n",
      "when input is [0, 14, 59, 58, 1, 44, 53] target is 56\n",
      "when input is [0, 14, 59, 58, 1, 44, 53, 56] target is 1\n",
      "when input is [60] target is 43\n",
      "when input is [60, 43] target is 56\n",
      "when input is [60, 43, 56] target is 1\n",
      "when input is [60, 43, 56, 1] target is 61\n",
      "when input is [60, 43, 56, 1, 61] target is 47\n",
      "when input is [60, 43, 56, 1, 61, 47] target is 50\n",
      "when input is [60, 43, 56, 1, 61, 47, 50] target is 50\n",
      "when input is [60, 43, 56, 1, 61, 47, 50, 50] target is 1\n",
      "when input is [0] target is 25\n",
      "when input is [0, 25] target is 63\n",
      "when input is [0, 25, 63] target is 1\n",
      "when input is [0, 25, 63, 1] target is 54\n",
      "when input is [0, 25, 63, 1, 54] target is 56\n",
      "when input is [0, 25, 63, 1, 54, 56] target is 39\n",
      "when input is [0, 25, 63, 1, 54, 56, 39] target is 47\n",
      "when input is [0, 25, 63, 1, 54, 56, 39, 47] target is 57\n",
      "when input is [1] target is 53\n",
      "when input is [1, 53] target is 59\n",
      "when input is [1, 53, 59] target is 56\n",
      "when input is [1, 53, 59, 56] target is 1\n",
      "when input is [1, 53, 59, 56, 1] target is 25\n",
      "when input is [1, 53, 59, 56, 1, 25] target is 39\n",
      "when input is [1, 53, 59, 56, 1, 25, 39] target is 56\n",
      "when input is [1, 53, 59, 56, 1, 25, 39, 56] target is 41\n",
      "when input is [43] target is 56\n",
      "when input is [43, 56] target is 43\n",
      "when input is [43, 56, 43] target is 11\n",
      "when input is [43, 56, 43, 11] target is 1\n",
      "when input is [43, 56, 43, 11, 1] target is 63\n",
      "when input is [43, 56, 43, 11, 1, 63] target is 43\n",
      "when input is [43, 56, 43, 11, 1, 63, 43] target is 57\n",
      "when input is [43, 56, 43, 11, 1, 63, 43, 57] target is 6\n",
      "when input is [53] target is 59\n",
      "when input is [53, 59] target is 1\n",
      "when input is [53, 59, 1] target is 46\n",
      "when input is [53, 59, 1, 46] target is 47\n",
      "when input is [53, 59, 1, 46, 47] target is 57\n",
      "when input is [53, 59, 1, 46, 47, 57] target is 1\n",
      "when input is [53, 59, 1, 46, 47, 57, 1] target is 50\n",
      "when input is [53, 59, 1, 46, 47, 57, 1, 50] target is 47\n",
      "when input is [63] target is 1\n",
      "when input is [63, 1] target is 53\n",
      "when input is [63, 1, 53] target is 58\n",
      "when input is [63, 1, 53, 58] target is 46\n",
      "when input is [63, 1, 53, 58, 46] target is 43\n",
      "when input is [63, 1, 53, 58, 46, 43] target is 56\n",
      "when input is [63, 1, 53, 58, 46, 43, 56] target is 1\n",
      "when input is [63, 1, 53, 58, 46, 43, 56, 1] target is 42\n",
      "when input is [1] target is 47\n",
      "when input is [1, 47] target is 58\n",
      "when input is [1, 47, 58] target is 1\n",
      "when input is [1, 47, 58, 1] target is 57\n",
      "when input is [1, 47, 58, 1, 57] target is 46\n",
      "when input is [1, 47, 58, 1, 57, 46] target is 39\n",
      "when input is [1, 47, 58, 1, 57, 46, 39] target is 50\n",
      "when input is [1, 47, 58, 1, 57, 46, 39, 50] target is 50\n",
      "when input is [50] target is 53\n",
      "when input is [50, 53] target is 52\n",
      "when input is [50, 53, 52] target is 45\n",
      "when input is [50, 53, 52, 45] target is 47\n",
      "when input is [50, 53, 52, 45, 47] target is 52\n",
      "when input is [50, 53, 52, 45, 47, 52] target is 45\n",
      "when input is [50, 53, 52, 45, 47, 52, 45] target is 1\n",
      "when input is [50, 53, 52, 45, 47, 52, 45, 1] target is 58\n",
      "when input is [53] target is 58\n",
      "when input is [53, 58] target is 1\n",
      "when input is [53, 58, 1] target is 56\n",
      "when input is [53, 58, 1, 56] target is 43\n",
      "when input is [53, 58, 1, 56, 43] target is 60\n",
      "when input is [53, 58, 1, 56, 43, 60] target is 43\n",
      "when input is [53, 58, 1, 56, 43, 60, 43] target is 56\n",
      "when input is [53, 58, 1, 56, 43, 60, 43, 56] target is 43\n",
      "when input is [45] target is 52\n",
      "when input is [45, 52] target is 6\n",
      "when input is [45, 52, 6] target is 1\n",
      "when input is [45, 52, 6, 1] target is 39\n",
      "when input is [45, 52, 6, 1, 39] target is 52\n",
      "when input is [45, 52, 6, 1, 39, 52] target is 42\n",
      "when input is [45, 52, 6, 1, 39, 52, 42] target is 1\n",
      "when input is [45, 52, 6, 1, 39, 52, 42, 1] target is 58\n",
      "when input is [41] target is 53\n",
      "when input is [41, 53] target is 59\n",
      "when input is [41, 53, 59] target is 52\n",
      "when input is [41, 53, 59, 52] target is 58\n",
      "when input is [41, 53, 59, 52, 58] target is 56\n",
      "when input is [41, 53, 59, 52, 58, 56] target is 63\n",
      "when input is [41, 53, 59, 52, 58, 56, 63] target is 6\n",
      "when input is [41, 53, 59, 52, 58, 56, 63, 6] target is 1\n",
      "when input is [0] target is 19\n",
      "when input is [0, 19] target is 53\n",
      "when input is [0, 19, 53] target is 53\n",
      "when input is [0, 19, 53, 53] target is 42\n",
      "when input is [0, 19, 53, 53, 42] target is 1\n",
      "when input is [0, 19, 53, 53, 42, 1] target is 15\n",
      "when input is [0, 19, 53, 53, 42, 1, 15] target is 39\n",
      "when input is [0, 19, 53, 53, 42, 1, 15, 39] target is 54\n",
      "when input is [43] target is 0\n",
      "when input is [43, 0] target is 43\n",
      "when input is [43, 0, 43] target is 47\n",
      "when input is [43, 0, 43, 47] target is 45\n",
      "when input is [43, 0, 43, 47, 45] target is 46\n",
      "when input is [43, 0, 43, 47, 45, 46] target is 58\n",
      "when input is [43, 0, 43, 47, 45, 46, 58] target is 8\n",
      "when input is [43, 0, 43, 47, 45, 46, 58, 8] target is 1\n",
      "when input is [39] target is 50\n",
      "when input is [39, 50] target is 50\n",
      "when input is [39, 50, 50] target is 5\n",
      "when input is [39, 50, 50, 5] target is 42\n",
      "when input is [39, 50, 50, 5, 42] target is 1\n",
      "when input is [39, 50, 50, 5, 42, 1] target is 59\n",
      "when input is [39, 50, 50, 5, 42, 1, 59] target is 54\n",
      "when input is [39, 50, 50, 5, 42, 1, 59, 54] target is 8\n",
      "when input is [43] target is 1\n",
      "when input is [43, 1] target is 47\n",
      "when input is [43, 1, 47] target is 57\n",
      "when input is [43, 1, 47, 57] target is 1\n",
      "when input is [43, 1, 47, 57, 1] target is 53\n",
      "when input is [43, 1, 47, 57, 1, 53] target is 44\n",
      "when input is [43, 1, 47, 57, 1, 53, 44] target is 1\n",
      "when input is [43, 1, 47, 57, 1, 53, 44, 1] target is 56\n",
      "when input is [47] target is 52\n",
      "when input is [47, 52] target is 1\n",
      "when input is [47, 52, 1] target is 46\n",
      "when input is [47, 52, 1, 46] target is 43\n",
      "when input is [47, 52, 1, 46, 43] target is 56\n",
      "when input is [47, 52, 1, 46, 43, 56] target is 1\n",
      "when input is [47, 52, 1, 46, 43, 56, 1] target is 43\n",
      "when input is [47, 52, 1, 46, 43, 56, 1, 43] target is 63\n",
      "when input is [52] target is 45\n",
      "when input is [52, 45] target is 57\n",
      "when input is [52, 45, 57] target is 6\n",
      "when input is [52, 45, 57, 6] target is 0\n",
      "when input is [52, 45, 57, 6, 0] target is 26\n",
      "when input is [52, 45, 57, 6, 0, 26] target is 53\n",
      "when input is [52, 45, 57, 6, 0, 26, 53] target is 58\n",
      "when input is [52, 45, 57, 6, 0, 26, 53, 58] target is 1\n",
      "when input is [53] target is 1\n",
      "when input is [53, 1] target is 40\n",
      "when input is [53, 1, 40] target is 43\n",
      "when input is [53, 1, 40, 43] target is 44\n",
      "when input is [53, 1, 40, 43, 44] target is 53\n",
      "when input is [53, 1, 40, 43, 44, 53] target is 56\n",
      "when input is [53, 1, 40, 43, 44, 53, 56] target is 43\n",
      "when input is [53, 1, 40, 43, 44, 53, 56, 43] target is 1\n",
      "when input is [10] target is 0\n",
      "when input is [10, 0] target is 32\n",
      "when input is [10, 0, 32] target is 46\n",
      "when input is [10, 0, 32, 46] target is 43\n",
      "when input is [10, 0, 32, 46, 43] target is 63\n",
      "when input is [10, 0, 32, 46, 43, 63] target is 6\n",
      "when input is [10, 0, 32, 46, 43, 63, 6] target is 1\n",
      "when input is [10, 0, 32, 46, 43, 63, 6, 1] target is 44\n",
      "when input is [52] target is 41\n",
      "when input is [52, 41] target is 43\n",
      "when input is [52, 41, 43] target is 6\n",
      "when input is [52, 41, 43, 6] target is 1\n",
      "when input is [52, 41, 43, 6, 1] target is 44\n",
      "when input is [52, 41, 43, 6, 1, 44] target is 53\n",
      "when input is [52, 41, 43, 6, 1, 44, 53] target is 56\n",
      "when input is [52, 41, 43, 6, 1, 44, 53, 56] target is 1\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'when input is {context.tolist()} target is {target}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ed48467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[43,  1, 54,  ...,  1, 50, 43],\n",
       "        [51, 43, 39,  ..., 50, 53, 60],\n",
       "        [46,  1, 47,  ..., 46, 53, 59],\n",
       "        ...,\n",
       "        [53,  1, 40,  ..., 32, 46, 43],\n",
       "        [10,  0, 32,  ..., 58,  1, 40],\n",
       "        [52, 41, 43,  ..., 60, 43, 56]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "364fc2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensors of ints\n",
    "        logits = self.token_embedding_table(idx)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "            logits, loss = self(idx) # gives (B,T,C)\n",
    "            logits = logits[:,-1, :] # we care only about the last time step, we get (B, T, C) --> (B,C)\n",
    "            probs = F.softmax(logits, dim=-1) # last dim\n",
    "            next_idx = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ddc03f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "927c605d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "egbeAhI$pbFPFVYdi'OPS;M 'yZ!bkownZPW:owLduQkUJ-!-QPqvCgJ'!!SS-F ELAZjUW?VA'WBs'ITbHnwcJLLWaW-Vs,&:bs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v3/cchdhwjd2p550zspwxwj6jlw0000gn/T/ipykernel_96749/2939767912.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(decode(model.generate(torch.tensor(torch.zeros((1,1)), dtype=torch.long), 100)[0].tolist()))\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.tensor(torch.zeros((1,1)), dtype=torch.long), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "627275f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.42185640335083\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "max_steps = 100000\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr)\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(max_steps):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5bb4a36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LAn ormay ucearant hesis bll.\n",
      "hequpr t outhyrat'd.\n",
      "IIUSI atild adount.\n",
      "\n",
      "Figequ, diente.\n",
      "\n",
      "Wherann th \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v3/cchdhwjd2p550zspwxwj6jlw0000gn/T/ipykernel_96749/2939767912.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(decode(model.generate(torch.tensor(torch.zeros((1,1)), dtype=torch.long), 100)[0].tolist()))\n"
     ]
    }
   ],
   "source": [
    "print(decode(model.generate(torch.tensor(torch.zeros((1,1)), dtype=torch.long), 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc86c825",
   "metadata": {},
   "source": [
    "### Attention - the mathematical trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f459fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication can be used to get weighted aggregation\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a/torch.sum(a, dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3,2)).float()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "11a92c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a -> \n",
      " tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b -> \n",
      " tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "a @ b -> \n",
      " tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "print(\"a -> \\n\", a)\n",
    "print(\"b -> \\n\", b)\n",
    "print(\"a @ b -> \\n\", a @ b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ee046d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B,T,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6ee608c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0431, -1.6047],\n",
       "         [ 1.7878, -0.4780],\n",
       "         [-0.2429, -0.9342],\n",
       "         [-0.2483, -1.2082],\n",
       "         [-0.7688,  0.7624],\n",
       "         [-1.5673, -0.2394],\n",
       "         [ 2.3228, -0.9634],\n",
       "         [ 2.0024,  0.4664]],\n",
       "\n",
       "        [[ 0.8008,  1.6806],\n",
       "         [ 0.3559, -0.6866],\n",
       "         [-0.4934,  0.2415],\n",
       "         [-1.1109,  0.0915],\n",
       "         [-0.2516,  0.8599],\n",
       "         [-0.3097, -0.3957],\n",
       "         [ 0.8034, -0.6216],\n",
       "         [-0.5920, -0.0631]],\n",
       "\n",
       "        [[ 0.3057, -0.7746],\n",
       "         [ 0.0349,  0.3211],\n",
       "         [ 1.5736, -0.8455],\n",
       "         [ 1.3123,  0.6872],\n",
       "         [-1.2347, -0.4879],\n",
       "         [-1.4181,  0.8963],\n",
       "         [ 0.0499,  2.2667],\n",
       "         [ 1.1790, -0.4345]],\n",
       "\n",
       "        [[-0.8140, -0.7360],\n",
       "         [-0.8371, -0.9224],\n",
       "         [ 1.8113,  0.1606],\n",
       "         [ 0.3672,  0.1754],\n",
       "         [-1.1845,  1.3835],\n",
       "         [-1.2024,  0.7078],\n",
       "         [-1.0759,  0.5357],\n",
       "         [ 1.1754,  0.5612]]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "92beda0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want x[b,t] = mean_{i<=t} xb[b,i]\n",
    "# version 1 - naive mean\n",
    "xbow = torch.zeros(B,T,C)\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1]\n",
    "        xbow[b, t] = torch.mean(xprev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "5b319d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2 - using matrix multiplication for weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei/wei.sum(dim=1, keepdim=True)\n",
    "xbow2 = wei @ x # (B,T,T) @ (B,T,C) -> (B, T, C) batched matrix multiply\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b77b91b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3 - using softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = tril.masked_fill(tril == 0, float('-inf'))\n",
    "wei = wei.softmax(dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow3, xbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f9c3969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 4 - self attention\n",
    "B,T,C = 4,8,32\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# single head performing self attention\n",
    "head_size = 16\n",
    "key = torch.nn.Linear(C, head_size, bias=False)\n",
    "query = torch.nn.Linear(C, head_size, bias=False)\n",
    "value = torch.nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "wei = q @ k.transpose(-2,-1) # (B,T,16) @ (B,16,T) -> (B,T,T)\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = wei.softmax(dim=1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "#out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "dab36cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f28f3e4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_embed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v3/cchdhwjd2p550zspwxwj6jlw0000gn/T/ipykernel_96749/3430270551.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mn_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'n_embed' is not defined"
     ]
    }
   ],
   "source": [
    "n_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8e92e44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16, 8])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.transpose(-1, -2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "4535f202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(0, len(data) - batch_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1: i+1+block_size] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss():\n",
    "    gpt.eval()\n",
    "    out = {}\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for i in range(eval_iters):\n",
    "            xb, yb = get_batch(split)\n",
    "            logits, loss = gpt(xb, yb)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    gpt.train()\n",
    "    return out\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "523e92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 64 # token and position embedding size\n",
    "block_size = 32 # context length\n",
    "dropout = 0.0\n",
    "n_layer = 4 # number of transformer blocks\n",
    "device = 'cpu'\n",
    "n_head = 4\n",
    "eval_iters = 200\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"one head of self attention\"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x) # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute affinities (attention)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B,T,C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B,T,T)\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei) # (B,T,T)\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B,T,T) @ (B,T,C) -> (B,T,C)\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parrallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concat across last dim\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transoformer block - communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "        \n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb #(B,T,C)\n",
    "        x = self.blocks(x) #(B,T,C)\n",
    "        x = self.ln_f(x) #(B,T,C)\n",
    "        logits = self.lm_head(x) #(B,T, vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_tokens):\n",
    "        # idx is (B,T)\n",
    "        for _ in range(max_tokens):\n",
    "            idx_context = idx[:,-block_size:] # crop context to block_size\n",
    "            logits, _ = self(idx_context)\n",
    "            # consider only the last time step\n",
    "            logits = logits[:,-1,:] # becomes (B,C)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        return idx\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "9c9eb12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iter=0 train loss=4.180072 val loss=4.174206\n",
      "\n",
      "Iter=1000 train loss=1.957292 val loss=2.022863\n",
      "\n",
      "Iter=2000 train loss=1.737550 val loss=1.893850\n",
      "\n",
      "Iter=3000 train loss=1.647651 val loss=1.821699\n",
      "\n",
      "Iter=4000 train loss=1.604235 val loss=1.776476\n",
      "\n",
      "Iter=5000 train loss=1.572132 val loss=1.738808\n",
      "\n",
      "Iter=6000 train loss=1.547580 val loss=1.715767\n",
      "\n",
      "Iter=7000 train loss=1.523399 val loss=1.704546\n",
      "\n",
      "Iter=8000 train loss=1.511895 val loss=1.693417\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v3/cchdhwjd2p550zspwxwj6jlw0000gn/T/ipykernel_96749/1386943587.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v3/cchdhwjd2p550zspwxwj6jlw0000gn/T/ipykernel_96749/1088326530.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mpos_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_emb\u001b[0m \u001b[0;31m#(B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(B,T, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/venv/lib/python3.10/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v3/cchdhwjd2p550zspwxwj6jlw0000gn/T/ipykernel_96749/1088326530.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v3/cchdhwjd2p550zspwxwj6jlw0000gn/T/ipykernel_96749/1088326530.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# concat across last dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v3/cchdhwjd2p550zspwxwj6jlw0000gn/T/ipykernel_96749/1088326530.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# concat across last dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/venv/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/v3/cchdhwjd2p550zspwxwj6jlw0000gn/T/ipykernel_96749/1088326530.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;31m# (B,T,C) @ (B, C, T) -> (B, T, T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwei\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtril\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwei\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mwei\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwei\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B,T,C)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/venv/lib/python3.10/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2140\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gpt = GPTLanguageModel()\n",
    "gpt = gpt.to(device)\n",
    "lr = 1e-3\n",
    "max_steps = 100000\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr)\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000== 0:\n",
    "        losses = evaluate_loss()\n",
    "        print(\"\\nIter={} train loss={:2f} val loss={:2f}\".format(i, losses['train'], losses['test']))\n",
    "        # sample from the model\n",
    "        #print(decode(gpt.generate(torch.zeros((1,1)).long(), 100).tolist()[0]))\n",
    "    lossi.append(loss.item())\n",
    "\n",
    "#print(loss.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "a384e365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First Clifford, In upon Tybalter fly.\n",
      "My eyesick of York must would no hand!\n",
      "Weepons these clameling, from so much I have that me ghost call'nger thy king them hearts they be affliction. Well thee woman,\n",
      "Well me grow with him of his know, Come, for woman, it whom I lest,\n",
      "And as in a state night, look how should with be thou hast unreason so is our that seeks the so request, her an merry shake power dies then?\n",
      "\n",
      "QUEEN MAPERLARET:\n",
      "Can Est he claughter'd me to be, that he count\n",
      "and fairewelow't own while, master good to drown;\n",
      "For so she we, not throng are do Romeo,\n",
      "And where how I am little but nor the blow:\n",
      "Ay, feect than userried of his knows in a maid with he thank\n",
      "That it all opense her farls you frown.\n",
      "\n",
      "ABREKEL:\n",
      "Sir, Polining Of EDWARD:\n",
      "Your love but when I, queen thy royal,\n",
      "The reepInect and warranced upon lady shy stand\n",
      "To where your clack his slip.\n",
      "\n",
      "GLORDIVERS:\n",
      "Ay, know, and and rubour Mainton,\n",
      "And or I who duke drests never you,\n",
      "Sir, and my swear Romeo,? and proads,\n",
      "Let esment-no\n"
     ]
    }
   ],
   "source": [
    "# generate samples\n",
    "print(decode(gpt.generate(torch.zeros((1,1)).long(), 1000).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f9c8bc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((3,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "bbedc99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2081, 0.1857, 0.1532, 0.7946, 0.0362, 0.7352, 0.0390, 0.6962],\n",
       "        [0.9008, 0.1008, 0.7824, 0.2185, 0.4749, 0.9446, 0.0310, 0.4892],\n",
       "        [0.5252, 0.9241, 0.9331, 0.2598, 0.6783, 0.3642, 0.3723, 0.3621]])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,-8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "b0b45e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4978, 0.8541, 0.2081, 0.1857, 0.1532, 0.7946, 0.0362, 0.7352],\n",
       "        [0.0286, 0.9920, 0.9008, 0.1008, 0.7824, 0.2185, 0.4749, 0.9446],\n",
       "        [0.1533, 0.2051, 0.5252, 0.9241, 0.9331, 0.2598, 0.6783, 0.3642]])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "26a1e3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4978, 0.8541, 0.2081, 0.1857, 0.1532, 0.7946, 0.0362, 0.7352, 0.0390,\n",
       "         0.6962],\n",
       "        [0.0286, 0.9920, 0.9008, 0.1008, 0.7824, 0.2185, 0.4749, 0.9446, 0.0310,\n",
       "         0.4892],\n",
       "        [0.1533, 0.2051, 0.5252, 0.9241, 0.9331, 0.2598, 0.6783, 0.3642, 0.3723,\n",
       "         0.3621]])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52011708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
